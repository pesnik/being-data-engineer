#+BEGIN_QUOTE
1. A Unified Analytics Engine
2. It's genesis, inspiration and adoption
#+END_QUOTE

*Keywords*: GFS, MapReduce, BigTable, data locality, cluster rack affinity

This chapter talks about the main components of the spark project and spark's distributed architecture.

The background goes like: Google at it's urgency to handle large volume of data had introduced the Google File System (GFS), MapReduce, BigTable. Taking inspiration from their published papers yahoo engineers came up with Hadoop File System and donated this to Apache. The shortcomings of MapReduce which are 1. hard to manage and administer with operational complexity 2. It's verbose API and lot of boilerplate setup code and 3. Intermediate computed results stored on the local disk affecting performance. To handle these engineers developed bespoke systems (Apache Hive, Apache Strom, Apache Impala, Apache Giraph, Apache Drill, Apache Malhout etc.). But all of them had their own learning curve with operational complexity. So Spark was introduced.

Apache Hadoop Framework: Hadoop Common, MapReduce, HDFS, and Apache Hadoop YARN.

#+BEGIN_QUOTE
Simple things should be simple, complex things should be possible -- Alan Kay
#+END_QUOTE

Spark's design philosophy centers around four key characteristics:
    1. Speed

       Spark pursued this by
        - Todays servers come cheap
        - Spark builds it's query computations as DAG. The DAG schedular and query optimizer constructs effective computational graph
        - Tungsten, physical execution engine, uses whole-stage code generation to generate compact code for execution.
    2. Ease of use

       provides a fundamental abstraction of a simple logical data structure called RDD. provides a set of transformation and actions which offers a simple programming model to build apps in familiar languages.
    3. Modularity

       Spark SQL, Spark Structured Streaming, Spark MLlib, and GraphX
    4. Extensibility

       Spark focuses on it's fast, parallel computation engine rather than on storage.

We use APIs to write spark application and spark converts this into a DAG that is executed by the core engine.

Spark's components:
    - *driver*: the orchestrator, orchestrates paraller operations on the cluster. Access the distributed components (cluster manager and spark executor) through a SparkSession.

      Key responsibilites:
        - instantiating a SparkSession
        - communicates with the cluster manager
        - requests resoureces from the cluster manager for executors
        - transforms all the spark operations into DAG computations, schedules them
        - distributes their execution as tasks across the spark executors.
        - communicates directly with the executors (after resoureces are allocated)
    - *SparkSession*:

      Not only did it subsume previous entry points to Spark like the SparkContext, SQLContext, HiveContext, SparkConf, and StreamingContext, but it also made working with Spark simpler and easier.

    - *Cluster Manager*:

      responsible for managing and allocationg resources. Currently supports standalone, yarn, mesos and kubernetes.

    - *Spark Executors*:

      runs on each worker node. In most deployment mode, only a single executor runs per node.

*Deployment Modes*: local, standalone, yarn(client), yarn(cluster), kubernetes.

** Distributed data and partitions
Though this is not always possible, each Spark executor is preferably allocated a task that requires it to read the partition closest to it in the network, observing data locality.

Partitioning allows for efficient parallelism. A distributed scheme of breaking up data into chunks or partitions allows Spark executors to process only data that is close to them, minimizing network bandwidth.

** The WH Questions
- what is apache spark?
- how all the components of Sparkâ€™s distributed architecture work together and communicate?
- what deployment modes are available?
