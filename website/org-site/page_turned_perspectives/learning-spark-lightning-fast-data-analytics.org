#+title:Learning Spark: Lightning-Fast Data Analytics
#+description: Book Review - Build your pipelines with Apache Airflow
#+author: r_hasan
#+date:<2024-01-05 Fri>
#+hugo_base_dir: ../../


#+BEGIN_QUOTE
***data engineers*** will learn how to use *Spark’s Structured APIs* to perform
complex data exploration and analysis on both batch and streaming data; use *Spark
SQL* for interactive queries; use Spark’s built-in and external *data sources* to read,
refine, and write data in different file formats as part of their extract, transform, and
load (ETL) tasks; and build reliable data lakes with Spark and the open source *Delta
Lake table format*.

for ***data scientists and machine learning engineers***, Spark’s *MLlib library* offers many
common algorithms to build distributed machine learning models. This book covers
how to build pipelines with MLlib, best practices for distributed machine learning, how to use Spark to scale single-node models, and how to manage and deploy these models using the open source library *MLflow*.
#+END_QUOTE

* Chapter 1, Introduction to Apache Spark: A Unified Analytics Engine
#+BEGIN_QUOTE
1. A Unified Analytics Engine
2. It's genesis, inspiration and adoption
#+END_QUOTE

*Keywords*: GFS, MapReduce, BigTable, data locality, cluster rack affinity

This chapter talks about the main components of the spark project and spark's distributed architecture.

The background goes like: Google at it's urgency to handle large volume of data had introduced the Google File System (GFS), MapReduce, BigTable. Taking inspiration from their published papers yahoo engineers came up with Hadoop File System and donated this to Apache. The shortcomings of MapReduce which are 1. hard to manage and administer with operational complexity 2. It's verbose API and lot of boilerplate setup code and 3. Intermediate computed results stored on the local disk affecting performance. To handle these engineers developed bespoke systems (Apache Hive, Apache Strom, Apache Impala, Apache Giraph, Apache Drill, Apache Malhout etc.). But all of them had their own learning curve with operational complexity. So Spark was introduced.

Apache Hadoop Framework: Hadoop Common, MapReduce, HDFS, and Apache Hadoop YARN.

#+BEGIN_QUOTE
Simple things should be simple, complex things should be possible -- Alan Kay
#+END_QUOTE

Spark's design philosophy centers around four key characteristics:
    1. Speed

       Spark pursued this by
        - Todays servers come cheap
        - Spark builds it's query computations as DAG. The DAG schedular and query optimizer constructs effective computational graph
        - Tungsten, physical execution engine, uses whole-stage code generation to generate compact code for execution.
    2. Ease of use

       provides a fundamental abstraction of a simple logical data structure called RDD. provides a set of transformation and actions which offers a simple programming model to build apps in familiar languages.
    3. Modularity

       Spark SQL, Spark Structured Streaming, Spark MLlib, and GraphX
    4. Extensibility

       Spark focuses on it's fast, parallel computation engine rather than on storage.

We use APIs to write spark application and spark converts this into a DAG that is executed by the core engine.

** The WH Questions

* Chapter 2, Downloading Apache Spark and Getting Started
* Chapter 3, Apache Spark’s Structured APIs through Chapter 6, Spark SQL and Datasets
* Chapter 7, Optimizing and Tuning Spark Applications
* Chapter 8, Structured Streaming
* Chapter 9, Building Reliable Data Lakes with Apache Spark
* Chapter 10, Machine Learning with MLlib
* Chapter 11, Managing, Deploying, and Scaling Machine Learning Pipelines with Apache Spark
* Chapter 12, Epilogue: Apache Spark 3.0
